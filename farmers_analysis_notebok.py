# -*- coding: utf-8 -*-
"""Farmers_Analysis Notebok.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V015lm0-kr_TgXhyLLmEtKJDASrrNp-y
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark import ml
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
from pyspark.sql.functions import when, col, isnan, count, mean
import pyspark.sql.types as pyspark_type

# Create a SparkSession
spark_sess= SparkSession.builder.appName("Farmers Market Analysis").getOrCreate()

dataframe = spark_sess.read.csv("/content/data/farmers_markets_from_usda.csv", header=True, inferSchema=True)

dataframe.show()

# Print the number of rows in the dataframe
print('Number of rows:', dataframe.count())

# Print the number of columns in the dataframe
print('Number of columns:', len(dataframe.columns))

dataframe.describe().show()

#drop the columns with more than 40% null data
dataframe = dataframe.select([col_name for col_name in dataframe.columns if dataframe.select(col(col_name)).na.drop().count() / dataframe.count() >= 0.4])

dataframe.show()

dataframe.select([count(when(isnan(dataframe_col) | col(dataframe_col).isNull(), dataframe_col)).alias(dataframe_col) for dataframe_col in dataframe.columns]).show()

d1 = dataframe.drop_duplicates().count()
d2 = dataframe.count()
print('Total number of duplicate rows', d2  - d1)
dataframe = dataframe.drop_duplicates()

dataframe.show(2)

# List of column names to be processed
columns = ["Credit", 'WIC', 'WICcash', 'SFMNP', 'SNAP', 'Organic', 'Bakedgoods', 'Cheese', 'Crafts', 'Flowers', 'Eggs', 'Seafood', 'Herbs', 'Vegetables', 'Honey', 'Jams', 'Maple', 'Meat', 'Nursery',
           'Plants', 'Poultry', 'Prepared', 'Soap', 'Trees', 'Wine', 'Coffee', 'Beans', 'Fruits', 'Grains', 'Juices', 'Mushrooms', 'PetFood', 'Tofu', 'WildHarvested', 'NUTS']

# Loop through each column to process
for cols_name in columns:
    # Apply transformation to the current column
    dataframe = dataframe.withColumn(cols_name,
                                     # Check if values are "Y", "true", or "True", replace with 1
                                     when(col(cols_name).isin("Y", "true", "True"), 1)
                                     # Check if values are "N", "False", "No", "NOT APPLICABLE", or "-", replace with 0
                                     .when(col(cols_name).isin("N", "False", "No", "NOT APPLICABLE", "-"), 0)
                                     # If value doesn't match any condition, set to None
                                     .otherwise(None))

dataframe.show()

# Loop through each column in the DataFrame
for col_name in dataframe.columns:
    # Get the data type of the current column
    col_dtype = dataframe.schema[col_name].dataType

    # Print the current column name
    print(f'Column: {col_name}')

    # Check the data type and apply appropriate handling
    if col_dtype in (int, float):
        # For numeric columns (int or float), fill null values with mean
        mean_value = 0  # You might want to calculate the actual mean value here
        dataframe = dataframe.withColumn(col_name, when(col(col_name).isNull(), mean_value).otherwise(col(col_name)))
    elif str(col_dtype) == 'DoubleType()':
        # For DoubleType columns, fill null values with most common value (mode)
        most_common_value = 0.0  # You might want to calculate the actual most common value here
        dataframe = dataframe.withColumn(col_name, when(col(col_name).isNull(), most_common_value).otherwise(col(col_name)))
    elif str(col_dtype) == 'StringType()':
        # For StringType columns, fill null values with 'NOT APPLICABLE'
        most_common_value = 'NOT APPLICABLE'
        dataframe = dataframe.withColumn(col_name, when(col(col_name).isNull(), most_common_value).otherwise(col(col_name)))
    elif str(col_dtype) == 'BoolType()':
        # For BoolType columns, fill null values with False
        most_common_value = False
        dataframe = dataframe.withColumn(col_name, when(col(col_name).isNull(), most_common_value).otherwise(col(col_name)))

dataframe = dataframe.dropna()

dataframe = dataframe.drop('FMID', 'MarketName','Website','Facebook', 'street', 'city', 'Twitter','Youtube','OtherMedia', 'Season1Date', 'Season1Time', 'Nuts', 'updateTime')

dataframe.show()

# Convert the 'zip' column to float data type
dataframe = dataframe.withColumn('zip', dataframe['zip'].cast('int'))

# Convert the 'x' column to float data type
dataframe = dataframe.withColumn('x', dataframe['x'].cast('float'))

# Convert the 'y' column to float data type
dataframe = dataframe.withColumn('y', dataframe['y'].cast('float'))

dataframe.show()

ml_string_columns = [dataframe_col for dataframe_col, col_dtype in dataframe.dtypes if col_dtype == 'string']

string_encoder = [StringIndexer(inputCol=single_col, outputCol=single_col+"_").setHandleInvalid("keep") for single_col in ml_string_columns]

pipeline = Pipeline(stages=string_encoder)

ml_dataframe = pipeline.fit(dataframe).transform(dataframe)

ml_col_to_use = [col + "_" for col in ml_string_columns]

ml_numerical_cols = [dataframe_col for dataframe_col, col_dtype in dataframe.dtypes if col_dtype in ["int", "float", "double"]]

ml_col_to_use += ml_numerical_cols

ml_dataframe.select(ml_col_to_use).show()

ml_dataframe = ml_dataframe.drop('WICcash', 'SFMNP', 'SNAP', 'WildHarvested', 'zip')

ml_col_to_use = list(set(ml_col_to_use).intersection(set(ml_dataframe.columns)))

ml_dataframe.select(ml_col_to_use).show()

dataframe_final = VectorAssembler(inputCols=ml_col_to_use, outputCol="features").transform(ml_dataframe)

dataframe_final.show()

(dataframe_train, dataframe_test) = dataframe_final.randomSplit([0.9, 0.1])
regression_r2_eval = RegressionEvaluator(labelCol="x", predictionCol="prediction", metricName="r2")

# Print a message indicating the start of linear regression training
print('Linear regression running')

# Train a Linear Regression model using the specified features and label
linear_model = LinearRegression(featuresCol='features', labelCol="x").fit(dataframe_train)

# Print a message indicating the prediction for the tested data
print('Prediction for data tested')

# Evaluate the Linear Regression model using R2 score on the test data and print the result
print("Linear regression model >> R2 score:", regression_r2_eval.evaluate(linear_model.transform(dataframe_test)))

# Print a message indicating the start of training the Random Forest model with training data
print('Training Random Forest model with train data')

# Train a Random Forest Regressor model using the specified features and label, with a specified number of bins
random_forest = RandomForestRegressor(featuresCol='features', labelCol="x", maxBins=7063).fit(dataframe_train)

# Print a message indicating prediction for the test data
print('Prediction for test data')

# Evaluate the Random Forest Regressor model using R2 score on the test data and print the result
print("Random regression forest model >> R2 score:", regression_r2_eval.evaluate(random_forest.transform(dataframe_test)))

# Print a message indicating the start of training the Gradient Boosting model with training data
print('Training Gradient Boosting model with train data')

# Train a Gradient Boosting Regressor model using the specified features and label, with a specified number of bins
gradient_boosting = GBTRegressor(featuresCol='features', labelCol="x", maxBins=7507).fit(dataframe_train)

# Print a message indicating prediction for the test data
print('Prediction for test data')

# Evaluate the Gradient Boosting Regressor model using R2 score on the test data and print the result
print("Gradient Boosting model >> R2 score:", regression_r2_eval.evaluate(gradient_boosting.transform(dataframe_test)))

